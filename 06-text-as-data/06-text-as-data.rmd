---
title: "Text as Data Tutorial â€” DSfE 2025"
author: "Irene Iodice"
date: "May 2025"
output: html_document
---

## Overview

In this class you will:
  
1. Learn basic text handling: regular expressions and text cleaning.
2. Apply similarity measures to text data.
3. Explore real-world WTO treaty data using text mining techniques.
4. Practice summarizing documents, generating word clouds, and comparing documents statistically (Chi-square test).
5. Visualize trends in treaty depth over time.

Be prepared to work hands-on with R, answer small guided questions, and reflect on how these techniques can be applied in your own research.

  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require("pacman")) install.packages("pacman"); library(pacman)
p_load(xml2, tidytext, textstem, wordcloud2, magrittr, tidyverse, here)
```

# Part 1: Regular Expressions + Similarity

*Goal:* learn how to clean and manipulate text strings.

## A. Regular expressions basics  
*What do regular expressions do? They allow you to **search and manipulate text patterns**.*
  
```{r regex}
grepl("class", "Today class it's very fun\n\n\t\t")
gsub("\t|\n", "", "Today class it's very fun\n\n\t\t")

some_text <- c("This", "topic", "is", "so", "much", "fun")
some_text[grep("^[T]", some_text)]

text_chunk <- "[This topic is not so fun]"
gsub("\\[|\\]", "", text_chunk)
```

**Exercise:**
  Extract all words starting with `p` from:

```{r}
c("Policy", "trade", "production", "data")
```

## B. Cosine similarity example  
*How similar are two texts? Cosine similarity quantifies it.*  
  
  **Question:** How does similarity change if you swap just one word?
  
```{r cosine}
s1 <- "The book is on the table"
s2 <- "The pen is on the table"
s3 <- "Put the pen on the book"
sv <- c(s1 = s1, s2 = s2, s3 = s3)
svs <- strsplit(tolower(sv), "\\s+")
termf <- table(stack(svs))
idf <- log(1 / rowMeans(termf != 0))
tfidf <- termf * idf
dp <- t(tfidf[,3]) %*% tfidf[,-3]
cosim <- dp / (sqrt(colSums(tfidf[,-3]^2)) * sqrt(sum(tfidf[,3]^2)))
cosim
```

**Exercise:**
  Create 3 new short sentences and compute pairwise cosine similarities.

# Part 2: Text Mining WTO Treaties

*Goal:* analyze real-world WTO treaty text data.

```{r download}
if (!dir.exists("input/tota-master")) {
  dir.create("input", showWarnings = FALSE)
  url <- "https://github.com/mappingtreaties/tota/archive/refs/heads/master.zip"
  download.file(url, "input/tota.zip")
  unzip("input/tota.zip", exdir = "input/")
}
```

## Read and analyze a sample XML file  
*You will extract treaty text from WTO XML files. What information are you most interested in?*
  
```{r treaty}
treaty_data <- read_xml("input/tota-master/xml/pta_1.xml")
# Step 2: Extract Information
# Convert XML data to tibble (data frame) and unnest the 'treaty' column
# Only keep rows where variable names (ie treaty_id) is 'date_signed' or 'parties_original'
info <- as_list(treaty_data) %>%
  tibble::as_tibble() %>%
  unnest_longer(treaty) %>%
  filter(treaty_id %in% c("date_signed", "parties_original"))

# Step 3: Extract Articles
# Find all XML nodes corresponding to 'article'
# Extract attributes and content, removing leading/trailing white space
articles <- treaty_data %>%
  xml_find_all("//article")
id <- articles %>%
  xml_attr("article_identifier") %>%
  as.character()
content <- articles %>%
  xml_text() %>%
  trimws()

# Step 4: Prepare Treaty Text Data Frame
# Create a data frame from the content and add year and parties columns
# Reorganize the columns and group by year and parties
# Summarize the content by pasting together the text of all articles for each group
treaty_text <- content %>%
  as.data.frame() %>%
  rename(content = ".") %>%
  mutate(year = unlist(filter(info, treaty_id=="date_signed")$treaty),
         parties = filter(info, treaty_id=="parties_original")$treaty) %>%
  ungroup() %>%
  select(content, year, parties) %>%
  group_by(year, parties) %>%
  summarise(treaty = paste(content, collapse = " // "), .groups = "keep")
```

## Most frequent words (with stopwords removed)  
*Identify the most common terms in a treaty. Why might you want to remove common stopwords?*
  
```{r freq}
# Find Most Frequent Words
# Convert the third column of the first row of 'treaty_text' to a data frame
# Unnest the tokens (words) in the 'treaty' column
# Count the frequency of each word and sort by frequency
# Display the top words (head() by default shows top 6)
treaty_text[1,3] %>% as.data.frame() %>% 
    unnest_tokens(word, treaty) %>%
    count(word, sort = TRUE) %>% head()
```
```{r}
# Do the same after removing stop words
data <- treaty_text[1,3] %>% 
  as.data.frame() %>% 
  unnest_tokens(word, treaty) %>%
  anti_join(stop_words) %>% 
  filter(!grepl("[0-9]", word)) %>%
  mutate(word = lemmatize_words(word)) %>% # Apply lemmatization here
  count(word, sort = TRUE) 
head(data)
```

**Exercise:**
  Plot top 10 words as a barplot.

## Wordcloud of treaty keywords  
*Visualize the most frequent treaty terms.*  
  
  **Question:** Which keywords dominate? Are they surprising?
  
```{r wordcloud}
wordcloud2(data, color = 'random-light', backgroundColor = "#152238")
```

## Comparing treaty content: Chi-square test  
*Statistically compare content of two treaties.*  
  
  **Question:** If the Chi-square test shows significance, what does that tell you about the two documents?
  
```{r read-my-xml}
read_my_xml <- function(x) {

  # Message to console about the file being processed
  print(paste0("Working on ", x, "."))
  
  # Step 1: Load XML data
  treaty_data <- read_xml(x)
  
  # Step 2: Extract and filter relevant information
  info <- as_list(treaty_data) %>%
    tibble::as_tibble() %>%
    unnest_longer(treaty) %>%
    filter(treaty_id %in% c("date_signed", "parties_original"))
  
  # Step 3: Extract Articles
  articles <- treaty_data %>% xml_find_all("//article")
  id <- articles %>% xml_attr("article_identifier") %>% as.character()
  content <- articles %>% xml_text() %>% trimws()
  
  # Step 4: Prepare data
  # dreate a data frame from the content, add year and parties 
  # columns, group and summarize
  data <- content %>%
    as.data.frame() %>%
    rename(content = ".") %>%
    mutate(year = unlist(filter(info, treaty_id=="date_signed")$treaty),
           parties = filter(info, treaty_id=="parties_original")$treaty) %>%
    ungroup() %>%
    select(content, year, parties) %>%
    group_by(year, parties) %>%
    summarise(treaty = paste(content, collapse = " // "), .groups = "keep")
  
  # Step 5: Text Analysis
  # Unnest tokens, remove stop words, count word frequency, 
  # and filter for "services" and "investments"
  temp <- data[1,3] %>%
    as.data.frame() %>%
    unnest_tokens(word, treaty) %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE) %>%
    ungroup() %>%
    mutate(tot_words = sum(n)) %>% 
    filter(grepl("^servic", word) | grepl("^agric", word)) %>%  # Match words starting with "agricult"
    mutate(n, tot_words, word, year = data$year, parties = data$parties)
  
  # If temp has no rows, create a default row
  if (nrow(temp) == 0) {
    temp <- data.frame(n = 0, tot_words = 0, word = "NA", year = data$year, parties = data$parties)
  }
  
  # add year and parties to the data frame
  data <- temp %>% mutate(year = data$year, parties = data$parties)
  # clean up environment by removing unnecessary objects
  rm(articles, id, content, treaty_data)
  
  return(data)
}

# Define the directory where the XML files are located and get a of all XML files in the specified directory
file_directory <- "input/tota-master/xml"
my_files <- list.files(file_directory, full.names = TRUE)
```


```{r chi-square}
set.seed(123)
draw2 <- sample(my_files, 2)

# Apply the 'read_my_xml' function to the two drawn files and bind the results into a single data frame
dat <- map_df(draw2, read_my_xml)

# Step 4: Reshape the data to a wide format where each 'word' has its own column
table <- dat %>% spread(key = word, value = n)
table

table <- table %>%
  mutate(agriculture_combined = rowSums(select(., starts_with("agric")), na.rm = TRUE)) %>%
  mutate(services_combined = rowSums(select(., starts_with("servic")), na.rm = TRUE)) %>% 
  select(-agricultural, -agriculture, -services, -service)  # Optionally drop the original columns
table

# Step 5: Conduct a Chi-square test on the 'investments' and 'services' columns
# 'correct = FALSE' applies no continuity correction which is a small adjustment 
# used in certain statistical tests
chi_sq_result <- chisq.test(table$agriculture_combined, table$services_combined, correct = FALSE)

# Print the chi-square result
print(chi_sq_result)
```

# Part 3: Depth of Trade Agreements Over Time

*Goal:* answer an economic research question using text data.  
*Do treaties become longer and more complex over time?*  
  
  **Question:** Does the trend match what you expected?
  
```{r}
file_directory <- "input/tota-master/xml"
my_files <- list.files(file_directory, full.names = TRUE)

read_my_xml <- function(x) {
  treaty_data <- read_xml(x)
  info <- as_list(treaty_data) %>% tibble::as_tibble() %>%
    unnest_longer(treaty) %>% filter(treaty_id %in% c("date_signed", "parties_original"))
  content <- treaty_data %>% xml_find_all("//article") %>%
    xml_text() %>% trimws()
  data.frame(year = unlist(filter(info, treaty_id == "date_signed")$treaty),
             words = length(unlist(strsplit(content, " "))))
}

dat <- map_df(my_files, read_my_xml)

dat %>% mutate(year = as.numeric(format(as.Date(year), "%Y"))) %>%
  group_by(year) %>%
  summarise(avg_words = mean(words, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = avg_words)) +
  geom_line() + geom_point(color = "red") +
  scale_x_continuous(breaks = seq(1950, 2020, 10)) +
  labs(title = "Depth of WTO Trade Agreements Over Time", x = "Year", y = "Avg Words") +
  theme_minimal()
```
